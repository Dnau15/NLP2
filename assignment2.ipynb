{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "file = open('big.txt').read()\n",
        "WORDS = Counter(get_words(file))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]).union(known(edits1(word))) or known(edits2(word)) or [word])\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import defaultdict\n",
        "\n",
        "bigrams = defaultdict(int)\n",
        "trigrams = defaultdict(int)\n",
        "fourgrams = defaultdict(int)\n",
        "fivegrams = defaultdict(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('w2_.txt', 'r') as file:  # Open the file 'w2_.txt' for reading\n",
        "    lines = file.read().splitlines()  # Read the lines of the file and split them\n",
        "    for line in lines:  # Iterate through each line in the file\n",
        "        line = line.strip().split('\\t')\n",
        "        \n",
        "        frequency = int(line[0])  # Extract the frequency value from the first element of the split line\n",
        "        \n",
        "        bigram = tuple(line[1:])  # Convert the rest of the line into a tuple, representing a bigram\n",
        "        \n",
        "        bigrams[bigram] += frequency  # Update the count of the bigram in the 'bigrams' dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('w5_.txt', 'r') as file:  # Open the file 'w5_.txt' for reading\n",
        "    lines = file.read().splitlines()  # Read the lines of the file and split them\n",
        "    for line in lines:  # Iterate through each line in the file\n",
        "        line = line.strip().split('\\t')  # Remove leading/trailing whitespaces and split the line by tab character\n",
        "        \n",
        "        frequency = int(line[0])  # Extract the frequency value from the first element of the split line\n",
        "        \n",
        "        fivegram = tuple(line[1:])  # Convert the rest of the line into a tuple, representing a fivegram\n",
        "        fourgram = [fivegram[:4], fivegram[1:]]  # Create two fourgrams from the fivegram\n",
        "        trigram = [fivegram[:3], fivegram[1:4], fivegram[2:]]  # Create three trigrams from the fivegram\n",
        "        \n",
        "        if fivegram not in fivegrams:  # Check if the fivegram is not already in the 'fivegrams' dictionary\n",
        "            fivegrams[fivegram] = frequency  # Add the fivegram to the 'fivegrams' dictionary with its frequency\n",
        "            \n",
        "        for t in trigram:  # Iterate through each trigram\n",
        "            trigrams[t] += frequency  # Increment the count of the trigram in the 'trigrams' dictionary\n",
        "            \n",
        "        for t in fourgram:  # Iterate through each fourgram\n",
        "            fourgrams[t] += frequency  # Increment the count of the fourgram in the 'fourgrams' dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_word(word, context):\n",
        "    # Limit the context to the last four words\n",
        "    context = context[-4:]\n",
        "    \n",
        "    freq_context = []\n",
        "    for i in range(1, 5):\n",
        "        cur_context = context[-i:]\n",
        "        \n",
        "        # Check if the subsentence exists in n-grams dictionaries and add its frequency to the list\n",
        "        subsentence = (*cur_context, word)\n",
        "        if i == 1 and subsentence in bigrams:\n",
        "            frequency = bigrams[subsentence]\n",
        "            freq_context.append((i, frequency))\n",
        "        elif i == 2 and subsentence in trigrams:\n",
        "            frequency = trigrams[subsentence]\n",
        "            freq_context.append((i, frequency))\n",
        "        elif i == 3 and subsentence in fourgrams:\n",
        "            frequency = fourgrams[subsentence]\n",
        "            freq_context.append((i, frequency))\n",
        "        elif i == 4 and subsentence in fivegrams:\n",
        "            frequency = fivegrams[subsentence]\n",
        "            freq_context.append((i, frequency))\n",
        "        \n",
        "    return freq_context\n",
        "            \n",
        "            \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(1, 8805), (2, 630), (3, 16), (4, 16)]"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_word(\"woods\", [\"a\", \"babe\", \"in\", \"the\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_max_context(contexts):\n",
        "    # Initialize variables to store the maximum length, maximum frequency, and index of the maximum context\n",
        "    max_len = 0\n",
        "    max_freq = 0\n",
        "    idx = -1\n",
        "    \n",
        "    # Iterate through each context in the list of contexts\n",
        "    for i in range(len(contexts)):\n",
        "        # Check if the length of the current context is greater than the maximum length found so far\n",
        "        if len(contexts[i]) > max_len:\n",
        "            # Update the maximum length, maximum frequency, and index of the maximum context\n",
        "            max_len = len(contexts[i])\n",
        "            max_freq = sum(c for _, c in contexts[i])  # Sum up the frequencies of all elements in the context\n",
        "            idx = i\n",
        "        # If the lengths are equal, compare the total frequencies\n",
        "        elif len(contexts[i]) == max_len:\n",
        "            # Calculate the total frequency of the current context\n",
        "            context_freq = sum(c for _, c in contexts[i])\n",
        "            # If the total frequency of the current context is greater than the maximum frequency found so far\n",
        "            if context_freq > max_freq:\n",
        "                # Update the maximum frequency and index of the maximum context\n",
        "                max_freq = context_freq\n",
        "                idx = i\n",
        "                \n",
        "    # Return the index of the context with the maximum length and highest total frequency\n",
        "    return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correct(word, context):\n",
        "    # Generate a list of candidate corrections for the misspelled word\n",
        "    candids = list(candidates(word))\n",
        "    \n",
        "    # Initialize lists to store new context with non-empty corrections and their corresponding indices\n",
        "    new_context = []\n",
        "    non_empty = []\n",
        "    \n",
        "    # Iterate through each candidate correction and its index\n",
        "    for i, cand in enumerate(candids):\n",
        "        # Find the corrected word corresponding to the candidate in the context\n",
        "        new_word = find_word(cand, context)\n",
        "        # If the corrected word is not empty, add it to the new context and store its index\n",
        "        if new_word != []:\n",
        "            new_context.append(new_word)\n",
        "            non_empty.append(i)\n",
        "    \n",
        "    # Find the index of the context with the maximum length and highest total frequency\n",
        "    idx = find_max_context(new_context)\n",
        "    \n",
        "    # If there are non-empty corrections available\n",
        "    if non_empty != []:\n",
        "        # Retrieve the corrected word corresponding to the index\n",
        "        correct_word = candids[non_empty[idx]]\n",
        "    else:\n",
        "        # If no non-empty corrections are found, use a default correction for the misspelled word\n",
        "        correct_word = correction(word)\n",
        "    \n",
        "    # Return the corrected word\n",
        "    return correct_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_text_with_mistakes = \\\n",
        "\"\"\"\"\n",
        "I red the book yesturday and it was really interessing.\n",
        "He's alwais late for meetings, it's so frustraiting.\n",
        "She baught a new dress for the occassion, it looks amasing on her.\n",
        "We had a deliscious dinner last night, with lot's of different dishes.\n",
        "Their cat is so cuite, with it's fluffy fur and big green eyes.\n",
        "I can't belive how bueatiful the sunset was yesterday.\n",
        "He's been working so hard latley, I hope he gets some rest soon.\n",
        "She's such a gr8 friend, always there when I need her.\n",
        "I herd the news about the accident, it was truely shoking.\n",
        "We're going too the beach this weakend, I can't wait to relax in the sun.\n",
        "\"\"\"\n",
        "\n",
        "test_text_corrected = \\\n",
        "\"\"\"\n",
        "I read the book yesterday and it was really interesting.\n",
        "He's always late for meetings, it's so frustrating.\n",
        "She bought a new dress for the occasion, it looks amazing on her.\n",
        "We had a delicious dinner last night, with lots of different dishes.\n",
        "Their cat is so cute, with its fluffy fur and big green eyes.\n",
        "I can't believe how beautiful the sunset was yesterday.\n",
        "He's been working so hard lately, I hope he gets some rest soon.\n",
        "She's such a great friend, always there when I need her.\n",
        "I heard the news about the accident, it was truly shocking.\n",
        "We're going to the beach this weekend, I can't wait to relax in the sun.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yesturday -> yesterday\n",
            "interessing -> interesting\n",
            "alwais -> always\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frustraiting -> frustraiting\n",
            "baught -> bought\n",
            "occassion -> occasion\n",
            "amasing -> amazing\n",
            "deliscious -> delicious\n",
            "cuite -> quite\n",
            "belive -> believe\n",
            "bueatiful -> beautiful\n",
            "latley -> lately\n",
            "gr8 -> grm\n",
            "truely -> truly\n",
            "shoking -> showing\n",
            "weakend -> weekend\n"
          ]
        }
      ],
      "source": [
        "for line in test_text_with_mistakes.splitlines():\n",
        "    words = get_words(line)\n",
        "    for i in range(len(words)):\n",
        "        if words[i] not in WORDS:\n",
        "            incorrect_word = words[i]\n",
        "            corrected_word = correct(words[i], words[:i])\n",
        "            words[i] = corrected_word\n",
        "            print(f\"{incorrect_word} -> {corrected_word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correct_mistakes(context):\n",
        "    # Iterate through each line in the context\n",
        "    for line in context.splitlines():\n",
        "        # Tokenize the line into words\n",
        "        words = get_words(line)\n",
        "        \n",
        "        # Iterate through each word in the line\n",
        "        for i in range(len(words)):\n",
        "            # Check if the word is not found in the pre-defined WORDS set (possibly a misspelling)\n",
        "            if words[i] not in WORDS:\n",
        "                # Correct the misspelled word based on the context before it\n",
        "                corrected_word = correct(words[i], words[:i])\n",
        "                # Replace the original word with the corrected one\n",
        "                words[i] = corrected_word\n",
        "                \n",
        "    # Reconstruct the context with corrected words and return it\n",
        "    return context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"\n",
            "I red the book yesturday and it was really interessing.\n",
            "He's alwais late for meetings, it's so frustraiting.\n",
            "She baught a new dress for the occassion, it looks amasing on her.\n",
            "We had a deliscious dinner last night, with lot's of different dishes.\n",
            "Their cat is so cuite, with it's fluffy fur and big green eyes.\n",
            "I can't belive how bueatiful the sunset was yesterday.\n",
            "He's been working so hard latley, I hope he gets some rest soon.\n",
            "She's such a gr8 friend, always there when I need her.\n",
            "I herd the news about the accident, it was truely shoking.\n",
            "We're going too the beach this weakend, I can't wait to relax in the sun.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(correct_mistakes(test_text_with_mistakes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I started with exploring the dataset. There were 3 provided datasets: big.txt, w2_.txt, w5_.txt. The first one consists of combined excerpts from public domain books sourced from Project Gutenberg, along with lists of the most commonly used words from Wiktionary and the British National Corpus, merged together. The second one contains bigrams, I decided to use them as they are. The third file consists of 5-grams. I decided to divide it into 3-grams, 4-grams and 5-grams. Using this method I can get more information about the dependencies in the text. Using n more than 5 in n-grams is not useful for me, because of lack of computational power. Therefore, I limited context to last four words. One of the biggest limitation of provided datasets is lack of knowledge where n-gram is located. There were no tokens such as \\<s> - start of sentence or <\\s> - end of sentence. Therefore we do not know location of the ngram and we cannot use this information for prediction. We assume that n-gram located somewhere in the center. However such a rough guess still got good results.\n",
        "\n",
        "I assumed that words that appear in a greater number of different contexts should be used more frequently. Once contexts are identified for all potential corrections, the algorithm picks the correction that appears in the context with the longest length and highest frequency. This approach is designed to give precedence to corrections backed by larger and more commonly occurring contexts, thereby enhancing the accuracy of spell correction.\n",
        "\n",
        "According to [1], there is no need to correct words with length less than 3. It does not make sense and hence computing suggestions and ranking them is\n",
        "not logical.\n",
        "\n",
        "For testing part, I synthesized dataset using edits1 and edits2 functions of Norving's solution. I assume that user misspelled a word 1-2 times. Also I assume that 1 type of error (edits1) is as possible as 2 type of error (edits2). Moreover, for simplicity, I synthesized mistakes only in the last word in the sentence. It is quite significant limitation, but for now it gives a good undestanding of model performance and gives impetus to the development of this method. Synthesizing error only in the last step guarantees that word has context.\n",
        "\n",
        "References: \\\n",
        "[1] - https://assets.amazon.science/81/43/a1eaa3814b009481d306deb69b02/scipub-1158.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('imdb_labelled.txt', 'r') as f:\n",
        "    test = [line.split('\\t')[0].strip() for line in f][:700]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def make_mistake(words):\n",
        "    # Randomly choose between two types of mistakes: edits1 or edits2\n",
        "    choice = random.randint(0, 1)\n",
        "    \n",
        "    # Generate incorrect words based on the chosen type of mistake\n",
        "    if choice == 0:\n",
        "        incorrect_words = edits1(words[-1])\n",
        "    elif choice == 1:\n",
        "        incorrect_words = edits2(words[-1])\n",
        "    \n",
        "    # Randomly select one incorrect word from the generated set of incorrect words\n",
        "    incorrect_word = random.choice(list(incorrect_words))\n",
        "    \n",
        "    # Replace the last word in the original list with the selected incorrect word\n",
        "    words[-1] = incorrect_word\n",
        "    \n",
        "    # Form an incorrect sentence by joining the modified list of words\n",
        "    incorrect_sentence = \" \".join(words)\n",
        "    \n",
        "    # Return the incorrect sentence along with the selected incorrect word\n",
        "    return incorrect_sentence, incorrect_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('i love lootball', 'lootball')"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "make_mistake(get_words(\"I love football\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test my solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7385714285714285\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for sentence in test:\n",
        "    words = get_words(sentence)\n",
        "    correct_word = words[-1]\n",
        "    incorrect_sentence, incorrect_word = make_mistake(words)\n",
        "    \n",
        "    corrected_word = correct(incorrect_word, words[:-1])\n",
        "    if corrected_word == correct_word:\n",
        "        count += 1\n",
        "\n",
        "print(count / len(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Norvig's solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6785714285714286\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for sentence in test:\n",
        "    words = get_words(sentence)\n",
        "    correct_word = words[-1]\n",
        "    incorrect_sentence, incorrect_word = make_mistake(words)\n",
        "    \n",
        "    corrected_word = correction(incorrect_word)\n",
        "    if corrected_word == correct_word:\n",
        "        count += 1\n",
        "\n",
        "print(count / len(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "According to the results, my solution is 7% better than Norvig's solution, because my solution uses context in contrast to Norvig's solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
