{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "file = open('big.txt').read()\n",
        "WORDS = Counter(get_words(file))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]).union(known(edits1(word))) or known(edits2(word)) or [word])\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import defaultdict\n",
        "\n",
        "bigrams = defaultdict(int)\n",
        "trigrams = defaultdict(int)\n",
        "fourgrams = defaultdict(int)\n",
        "fivegrams = defaultdict(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('w2_.txt', 'r') as file:\n",
        "    lines = file.read().splitlines()\n",
        "    for line in lines:\n",
        "        line = line.strip().split('\\t')\n",
        "        \n",
        "        frequency = int(line[0])\n",
        "        \n",
        "        bigram = tuple(line[1:])\n",
        "        \n",
        "        bigrams[bigram] += frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('w5_.txt', 'r') as file:\n",
        "    lines = file.read().splitlines()\n",
        "    for line in lines:\n",
        "        line = line.strip().split('\\t')\n",
        "        \n",
        "        frequency = int(line[0])\n",
        "        \n",
        "        fivegram = tuple(line[1:])\n",
        "        fourgram = [fivegram[:4], fivegram[1:]]\n",
        "        trigram = [fivegram[:3], fivegram[1:4], fivegram[2:]]\n",
        "        \n",
        "        if fivegram not in fivegrams:\n",
        "            fivegrams[fivegram] = frequency\n",
        "            \n",
        "        for t in trigram:\n",
        "            trigrams[t] += frequency\n",
        "            \n",
        "        for t in fourgram:\n",
        "            fourgrams[t] += frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rewrite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_word(word, context):\n",
        "    context = context[-4:]\n",
        "    \n",
        "    freq_context = []\n",
        "    for i in range(1, 5):\n",
        "        cur_context = context[-i:]\n",
        "        \n",
        "        subsentence = (*cur_context, word)\n",
        "        if i == 1 and subsentence in bigrams:\n",
        "            frequency = bigrams[subsentence]\n",
        "            freq_context.append((i, frequency))\n",
        "        elif i == 2 and subsentence in trigrams:\n",
        "            frequency = trigrams[subsentence]\n",
        "            freq_context.append((i, frequency))\n",
        "        elif i == 3 and subsentence in fourgrams:\n",
        "            frequency = fourgrams[subsentence]\n",
        "            freq_context.append((i, frequency))\n",
        "        elif i == 4 and subsentence in fivegrams:\n",
        "            frequency = fivegrams[subsentence]\n",
        "            freq_context.append((i, frequency))\n",
        "        \n",
        "    return freq_context\n",
        "            \n",
        "            \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(1, 8805), (2, 630), (3, 16), (4, 16)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_word(\"woods\", [\"a\", \"babe\", \"in\", \"the\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_max_context(contexts):\n",
        "    max_len = 0\n",
        "    max_freq = 0\n",
        "    idx = -1\n",
        "    for i in range(len(contexts)):\n",
        "        if len(contexts[i]) > max_len:\n",
        "            max_len = len(contexts[i])\n",
        "            max_freq = sum(c for _, c in contexts[i])\n",
        "            idx = i\n",
        "        elif len(contexts[i]) == max_len:\n",
        "            context_freq = sum(c for _, c in contexts[i])\n",
        "            if context_freq > max_freq:\n",
        "                max_freq = context_freq\n",
        "                idx = i\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correct(word, context):\n",
        "    candids = list(candidates(word))\n",
        "    \n",
        "    new_context = []\n",
        "    non_empty = []\n",
        "    for i, cand in enumerate(candids):\n",
        "        new_word = find_word(cand, context)\n",
        "        if new_word != []:\n",
        "            new_context.append(new_word)\n",
        "            non_empty.append(i)\n",
        "    \n",
        "    idx = find_max_context(new_context)\n",
        "    \n",
        "    if non_empty != []:\n",
        "        correct_word = candids[non_empty[idx]]\n",
        "    else:\n",
        "        correct_word = correction(word)\n",
        "    return correct_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_text_with_mistakes = \\\n",
        "\"\"\"\"\n",
        "I red the book yesturday and it was really interessing.\n",
        "He's alwais late for meetings, it's so frustraiting.\n",
        "She baught a new dress for the occassion, it looks amasing on her.\n",
        "We had a deliscious dinner last night, with lot's of different dishes.\n",
        "Their cat is so cuite, with it's fluffy fur and big green eyes.\n",
        "I can't belive how bueatiful the sunset was yesterday.\n",
        "He's been working so hard latley, I hope he gets some rest soon.\n",
        "She's such a gr8 friend, always there when I need her.\n",
        "I herd the news about the accident, it was truely shoking.\n",
        "We're going too the beach this weakend, I can't wait to relax in the sun.\n",
        "\"\"\"\n",
        "\n",
        "test_text_corrected = \\\n",
        "\"\"\"\n",
        "I read the book yesterday and it was really interesting.\n",
        "He's always late for meetings, it's so frustrating.\n",
        "She bought a new dress for the occasion, it looks amazing on her.\n",
        "We had a delicious dinner last night, with lots of different dishes.\n",
        "Their cat is so cute, with its fluffy fur and big green eyes.\n",
        "I can't believe how beautiful the sunset was yesterday.\n",
        "He's been working so hard lately, I hope he gets some rest soon.\n",
        "She's such a great friend, always there when I need her.\n",
        "I heard the news about the accident, it was truly shocking.\n",
        "We're going to the beach this weekend, I can't wait to relax in the sun.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yesturday -> yesterday\n",
            "interessing -> interesting\n",
            "alwais -> always\n",
            "frustraiting -> frustraiting\n",
            "baught -> bought\n",
            "occassion -> occasion\n",
            "amasing -> amazing\n",
            "deliscious -> delicious\n",
            "cuite -> quite\n",
            "belive -> believe\n",
            "bueatiful -> beautiful\n",
            "latley -> lately\n",
            "gr8 -> grm\n",
            "truely -> truly\n",
            "shoking -> showing\n",
            "weakend -> weekend\n"
          ]
        }
      ],
      "source": [
        "for line in test_text_with_mistakes.splitlines():\n",
        "    words = get_words(line)\n",
        "    for i in range(len(words)):\n",
        "        if words[i] not in WORDS:\n",
        "            incorrect_word = words[i]\n",
        "            corrected_word = correct(words[i], words[:i])\n",
        "            words[i] = corrected_word\n",
        "            print(f\"{incorrect_word} -> {corrected_word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correct_mistakes(context):\n",
        "    for line in context.splitlines():\n",
        "        words = get_words(line)\n",
        "        for i in range(len(words)):\n",
        "            if words[i] not in WORDS:\n",
        "                corrected_word = correct(words[i], words[:i])\n",
        "                words[i] = corrected_word\n",
        "    return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"\n",
            "I red the book yesturday and it was really interessing.\n",
            "He's alwais late for meetings, it's so frustraiting.\n",
            "She baught a new dress for the occassion, it looks amasing on her.\n",
            "We had a deliscious dinner last night, with lot's of different dishes.\n",
            "Their cat is so cuite, with it's fluffy fur and big green eyes.\n",
            "I can't belive how bueatiful the sunset was yesterday.\n",
            "He's been working so hard latley, I hope he gets some rest soon.\n",
            "She's such a gr8 friend, always there when I need her.\n",
            "I herd the news about the accident, it was truely shoking.\n",
            "We're going too the beach this weakend, I can't wait to relax in the sun.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(correct_mistakes(test_text_with_mistakes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "*Your text here...*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('imdb_labelled.txt', 'r') as f:\n",
        "    test = [line.split('\\t')[0].strip() for line in f][:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def make_mistake(words):\n",
        "    choise = random.randint(0,1)\n",
        "    if choise == 0:\n",
        "        incorrect_words = edits1(words[-1])\n",
        "    elif choise == 1:\n",
        "        incorrect_words = edits2(words[-1])\n",
        "    incorrect_word = random.choice(list(incorrect_words))\n",
        "    words[-1] = incorrect_word\n",
        "    incorrect_sentence = \" \".join(words)\n",
        "    return incorrect_sentence, incorrect_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('i love footballt', 'footballt')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "make_mistake(get_words(\"I love football\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test my solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.72\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for sentence in test:\n",
        "    words = get_words(sentence)\n",
        "    correct_word = words[-1]\n",
        "    incorrect_sentence, incorrect_word = make_mistake(words)\n",
        "    \n",
        "    corrected_word = correct(incorrect_word, words[:-1])\n",
        "    if corrected_word == correct_word:\n",
        "        count += 1\n",
        "\n",
        "print(count / len(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Norvig's solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.62\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for sentence in test:\n",
        "    words = get_words(sentence)\n",
        "    correct_word = words[-1]\n",
        "    incorrect_sentence, incorrect_word = make_mistake(words)\n",
        "    \n",
        "    corrected_word = correction(incorrect_word)\n",
        "    if corrected_word == correct_word:\n",
        "        count += 1\n",
        "\n",
        "print(count / len(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
